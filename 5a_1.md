# Explicación del Cuaderno: Modelos Preentrenados - Puerta para Perros Inteligente

Este cuaderno introduce el concepto de utilizar modelos de aprendizaje profundo preentrenados. Estos modelos han sido entrenados en grandes conjuntos de datos (como ImageNet) y pueden ser utilizados directamente para tareas de clasificación de imágenes o como base para tareas más específicas.

## 1. Introducción y Objetivos

*   **Celda de Markdown (Encabezado):**
    *   Muestra un encabezado gráfico.

*   **Celda de Markdown (Título Principal):**
    *   `# 5a. Pre-Trained Models`

*   **Celda de Markdown (Introducción a Modelos Preentrenados):**
    *   Explica que, aunque a menudo se necesitan grandes datasets, existen muchos modelos preentrenados disponibles.
    *   Sugiere buscar modelos existentes en lugares como NGC (NVIDIA GPU Cloud) o GitHub antes de comenzar un nuevo proyecto.

*   **Celda de Markdown (Objetivos):**
    *   Usar TorchVision para cargar un modelo preentrenado.
    *   Preprocesar imágenes propias para que funcionen con el modelo preentrenado.
    *   Usar el modelo preentrenado para realizar inferencias precisas en imágenes propias.

*   **Celda de Código (Importaciones y Configuración del Dispositivo):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    import torch
    import torchvision.transforms.v2 as transforms
    import torchvision.io as tv_io

    import json

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.cuda.is_available()
    ```
    *   **Explicación:**
        *   `torch`: Biblioteca principal de PyTorch.
        *   `torchvision.transforms.v2`: Módulo para transformaciones de imágenes (preprocesamiento, aumento).
        *   `torchvision.io`: Módulo para leer archivos de imagen y video.
        *   `json`: Para trabajar con archivos JSON (usado más adelante para las etiquetas de ImageNet).
        *   `device`: Configura el dispositivo para el entrenamiento/inferencia. Usará una GPU CUDA si está disponible (`"cuda"`), de lo contrario usará la CPU (`"cpu"`).
    *   **Posibles Modificaciones:**
        *   Si necesitas bibliotecas adicionales, agrégalas aquí.

## 2. Una Puerta para Perros Automatizada

Se presenta un escenario práctico: crear una puerta para perros que solo permita la entrada a perros.

*   **Celda de Markdown (Descripción del Problema):**
    *   El objetivo es construir una puerta que distinga perros de otros animales.
    *   Se menciona que entrenar un modelo desde cero requeriría un dataset muy grande.
    *   Se introduce la idea de usar un modelo preentrenado en ImageNet, que ya conoce muchas categorías de animales.
    *   ImageNet es un desafío y un dataset a gran escala con modelos que clasifican imágenes en 1000 categorías.

## 3. Cargando el Modelo

Se carga un modelo preentrenado (VGG16) desde TorchVision.

*   **Celda de Markdown (Instrucciones para Cargar):**
    *   Los modelos de ImageNet están disponibles en TorchVision.
    *   Se elige VGG16 con sus pesos por defecto.

*   **Celda de Código (Carga de VGG16):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    from torchvision.models import vgg16
    from torchvision.models import VGG16_Weights

    # load the VGG16 network *pre-trained* on the ImageNet dataset
    weights = VGG16_Weights.DEFAULT # Carga los pesos predeterminados (los mejores disponibles)
    model = vgg16(weights=weights) # Crea el modelo VGG16 con esos pesos
    ```
    *   **Explicación:**
        *   `from torchvision.models import vgg16, VGG16_Weights`: Importa la arquitectura del modelo VGG16 y la enumeración de sus pesos disponibles.
        *   `weights = VGG16_Weights.DEFAULT`: Selecciona los pesos predeterminados y recomendados para VGG16, que han sido entrenados en ImageNet.
        *   `model = vgg16(weights=weights)`: Instancia el modelo VGG16 y carga los pesos preentrenados.
    *   **Posibles Modificaciones:**
        *   Puedes elegir otro modelo de `torchvision.models` (ej. `resnet50`, `efficientnet_b0`) y sus correspondientes pesos (ej. `ResNet50_Weights.DEFAULT`).
        *   Puedes seleccionar una versión específica de los pesos si es necesario (ej. `VGG16_Weights.IMAGENET1K_V1`).

*   **Celda de Markdown (Visualización del Modelo):**
    *   Menciona que la estructura del modelo es similar a la CNN del ejercicio de lenguaje de señas.

*   **Celda de Código (Mover Modelo al Dispositivo y Mostrarlo):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    model.to(device) # Mueve el modelo al dispositivo configurado (GPU o CPU)
    # La celda original solo tiene model.to(device), pero para ver el modelo,
    # se imprimiría 'model' en una celda de código.
    # print(model) # Descomentar para ver la estructura del modelo
    ```
    *   **Explicación:**
        *   `model.to(device)`: Transfiere todos los parámetros y búferes del modelo al `device` especificado. Es crucial para la ejecución en GPU.
    *   **Posibles Modificaciones:** No se modifica directamente, pero es un paso esencial.

### 3.1 Dimensiones de Entrada

Las imágenes de entrada deben tener las dimensiones y el preprocesamiento que espera el modelo.

*   **Celda de Markdown (Explicación de Dimensiones de Entrada):**
    *   Las imágenes deben coincidir con las dimensiones de entrada esperadas por el modelo.
    *   Los pesos preentrenados (`weights`) vienen con sus propias transformaciones recomendadas.

*   **Celda de Código (Obtener Transformaciones de Preprocesamiento):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    pre_trans = weights.transforms() # Obtiene la secuencia de transformaciones asociadas a los pesos
    pre_trans # Muestra las transformaciones
    ```
    *   **Explicación:**
        *   `weights.transforms()`: Devuelve un objeto `Compose` que contiene la secuencia de transformaciones necesarias para preprocesar una imagen de entrada para el modelo VGG16 con los pesos `DEFAULT`. Esto asegura que las imágenes se preparen de la misma manera que durante el entrenamiento original del modelo.
    *   **Posibles Modificaciones:** Generalmente no se modifica, ya que estas son las transformaciones óptimas para los pesos cargados.

*   **Celda de Markdown (Equivalente de las Transformaciones):**
    *   Muestra el código Python equivalente a `weights.transforms()` para VGG16.
    *   Explica las nuevas transformaciones: `Normalize` (re-escala los colores de la imagen basado en la media y desviación estándar de ImageNet) y `CenterCrop` (recorta el centro de la imagen).
    ```python
    # Esto es una explicación en Markdown, no código ejecutable directo en esta celda.
    # IMG_WIDTH, IMG_HEIGHT = (224, 224)

    # pre_trans = transforms.Compose([
    #     transforms.ToDtype(torch.float32, scale=True), # Convierte a float32 y escala [0, 255] a [0, 1]
    #     transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),    # Redimensiona a 224x224
    #     transforms.Normalize(                          # Normaliza con la media y std de ImageNet
    #         mean=[0.485, 0.456, 0.406],
    #         std=[0.229, 0.224, 0.225],
    #     ),
    #     transforms.CenterCrop(224)                     # Recorta el centro a 224x224
    # ])
    ```
    *   **Explicación de las transformaciones individuales:**
        *   `ToDtype(torch.float32, scale=True)`: Convierte la imagen a tipo `float32` y escala los valores de píxeles del rango [0, 255] al rango [0, 1].
        *   `Resize((IMG_WIDTH, IMG_HEIGHT))`: Redimensiona la imagen a 224x224 píxeles, que es el tamaño de entrada esperado por VGG16.
        *   `Normalize(mean=[...], std=[...])`: Normaliza los canales de la imagen utilizando la media y desviación estándar con las que se entrenó el modelo en ImageNet. Esto es crucial para un buen rendimiento.
        *   `CenterCrop(224)`: Recorta la región central de la imagen a un tamaño de 224x224.

### 3.2 Dimensiones de Salida

El modelo VGG16 entrenado en ImageNet produce una salida de 1000 elementos.

*   **Celda de Markdown (Explicación de Dimensiones de Salida):**
    *   El modelo devuelve una predicción de 1000 elementos, cada uno correspondiente a una categoría de ImageNet.
    *   Se proporciona un enlace a la lista de categorías.
    *   Se especifican los rangos de índices para perros (151-268) y gatos (281-285) en estas 1000 categorías.

## 4. Cargando una Imagen

Se carga y muestra una imagen de ejemplo.

*   **Celda de Markdown (Instrucción):**
    *   Se cargará y mostrará una imagen.

*   **Celda de Código (Función `show_image`):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg

    def show_image(image_path):
        image = mpimg.imread(image_path) # Lee la imagen desde el archivo
        plt.imshow(image)                # Muestra la imagen
    ```
    *   **Explicación:**
        *   `matplotlib.pyplot` y `matplotlib.image`: Bibliotecas para visualización y carga de imágenes.
        *   `show_image(image_path)`: Una función simple para leer una imagen de una ruta y mostrarla.
    *   **Posibles Modificaciones:** Podrías añadir títulos, ejes, etc., a la visualización si es necesario.

*   **Celda de Código (Mostrar Imagen de Ejemplo):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    show_image("data/doggy_door_images/happy_dog.jpg")
    ```
    *   **Explicación:** Llama a `show_image` para mostrar la imagen de un perro feliz.
    *   **Posibles Modificaciones:** Cambia la ruta para mostrar otras imágenes.

### 4.1 Preprocesando la Imagen

La imagen cargada debe ser preprocesada para que coincida con el formato de entrada del modelo.

*   **Celda de Markdown (Instrucción de Preprocesamiento):**
    *   La imagen debe ser preprocesada para tener la forma final `(1, 3, 224, 224)` (lote de 1 imagen, 3 canales de color, 224x224 píxeles).
    *   Se usarán las transformaciones de `weights`.

*   **Celda de Código (Función `load_and_process_image`):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    def load_and_process_image(file_path):
        # Imprime la forma original de la imagen como referencia
        print('Original image shape: ', mpimg.imread(file_path).shape)
        
        # Lee la imagen usando torchvision.io (produce un tensor) y la mueve al dispositivo
        image = tv_io.read_image(file_path).to(device)
        # Aplica las transformaciones de preprocesamiento (obtenidas de weights.transforms())
        image = pre_trans(image)
        # Añade una dimensión de lote (batch) al principio: (C, H, W) -> (1, C, H, W)
        image = image.unsqueeze(0)
        return image
    ```
    *   **Explicación:**
        *   `tv_io.read_image(file_path).to(device)`: Lee la imagen como un tensor PyTorch (en formato C, H, W) y la envía al `device`.
        *   `image = pre_trans(image)`: Aplica la secuencia de transformaciones (`Resize`, `Normalize`, `CenterCrop`, etc.) definidas anteriormente.
        *   `image = image.unsqueeze(0)`: Añade una dimensión al principio del tensor. Los modelos de PyTorch esperan lotes de imágenes, por lo que una sola imagen `(C, H, W)` se convierte en un lote de una imagen `(1, C, H, W)`.
    *   **Posibles Modificaciones:** Si usas un modelo diferente con requisitos de preprocesamiento distintos, `pre_trans` cambiaría (generalmente obtenido de los `weights` del nuevo modelo).

*   **Celda de Markdown (Prueba de Preprocesamiento):**
    *   Se prueba la función con la imagen del perro feliz.

*   **Celda de Código (Procesar Imagen y Mostrar Forma):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    processed_image = load_and_process_image("data/doggy_door_images/happy_dog.jpg")
    print("Processed image shape: ", processed_image.shape)
    ```
    *   **Explicación:** Llama a `load_and_process_image` y verifica que la forma de la imagen procesada sea `(1, 3, 224, 224)`.

*   **Celda de Markdown (Visualización de Imagen Procesada):**
    *   Pregunta cómo se ve la imagen después del preprocesamiento.

*   **Celda de Código (Mostrar Imagen Procesada):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    import torchvision.transforms.functional as F

    # Para visualizar, necesitamos quitar la dimensión de lote y convertir a imagen PIL
    plot_image = F.to_pil_image(torch.squeeze(processed_image.cpu())) # .cpu() si está en GPU
    plt.imshow(plot_image, cmap='gray') # cmap='gray' no es ideal para imágenes a color normalizadas
    # plt.imshow(plot_image) # Sería mejor sin cmap='gray'
    ```
    *   **Explicación:**
        *   `torch.squeeze(processed_image.cpu())`: Elimina la dimensión de lote (de `(1, C, H, W)` a `(C, H, W)`) y mueve el tensor a la CPU si estaba en la GPU (necesario para `to_pil_image`).
        *   `F.to_pil_image(...)`: Convierte el tensor PyTorch a una imagen PIL, que `matplotlib` puede mostrar.
        *   La imagen se verá con colores extraños debido a la transformación `Normalize` y ligeramente recortada/zoomada por `CenterCrop`.
    *   **Posibles Modificaciones:** Para una mejor visualización de una imagen normalizada, se necesitaría invertir la normalización, lo cual no se hace aquí ya que el objetivo es solo verificar la forma. Usar `plt.imshow(plot_image)` sin `cmap='gray'` es más apropiado para imágenes a color.

## 5. Realizar una Predicción

Se utiliza el modelo preentrenado para clasificar la imagen procesada.

*   **Celda de Markdown (Introducción a la Predicción):**
    *   La salida del modelo será un array de 1000 elementos.
    *   Se cargará un archivo JSON que mapea los índices de salida a nombres de clases legibles.

*   **Celda de Código (Cargar Clases de ImageNet):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    vgg_classes = json.load(open("data/imagenet_class_index.json"))
    ```
    *   **Explicación:** Carga el archivo `imagenet_class_index.json`, que contiene un diccionario donde las claves son los índices (como strings, ej. "0", "1") y los valores son listas con un identificador y el nombre de la clase (ej. `['n01440764', 'tench']`).
    *   **Posibles Modificaciones:** Si usas un modelo entrenado en un conjunto de clases diferente, necesitarás un archivo de mapeo de clases correspondiente.

*   **Celda de Markdown (Formato del Archivo de Clases):**
    *   Muestra un ejemplo del formato del archivo JSON.

*   **Celda de Código (Ejemplo de Clase):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    vgg_classes["0"] # Muestra la entrada para la clase con índice 0
    ```

*   **Celda de Markdown (Función de Predicción Legible):**
    *   Se creará una función para obtener las predicciones principales de forma legible, usando `torch.topk`.

*   **Celda de Código (Función `readable_prediction`):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    def readable_prediction(image_path):
        # Muestra la imagen original
        show_image(image_path)
        # Carga y preprocesa la imagen
        image = load_and_process_image(image_path)
        
        # Pone el modelo en modo evaluación (importante para capas como Dropout, BatchNorm)
        model.eval()
        with torch.no_grad(): # Desactiva el cálculo de gradientes para la inferencia
            output = model(image)[0]  # Pasa la imagen por el modelo y quita la dimensión de lote de la salida
        
        # Obtiene las 3 predicciones principales (valores y sus índices)
        predictions = torch.topk(output, 3)
        indices = predictions.indices.tolist() # Convierte los índices a una lista Python
        
        # Imprime las predicciones en formato legible
        out_str = "Top results: "
        pred_classes = [vgg_classes[str(idx)][1] for idx in indices] # Mapea índices a nombres de clase
        out_str += ", ".join(pred_classes)
        print(out_str)

        return predictions
    ```
    *   **Explicación:**
        *   `model.eval()`: Pone el modelo en modo de evaluación. Esto es importante porque desactiva capas como Dropout y asegura que BatchNorm use las estadísticas aprendidas durante el entrenamiento, no las del lote actual.
        *   `with torch.no_grad()`: Deshabilita el cálculo de gradientes, lo que ahorra memoria y acelera la inferencia, ya que no se necesita para la predicción.
        *   `output = model(image)[0]`: Realiza la inferencia. `model(image)` devuelve un tensor de forma `(1, 1000)`, así que `[0]` selecciona el resultado para la única imagen del lote.
        *   `torch.topk(output, 3)`: Encuentra los 3 valores más altos (probabilidades o logits) en el tensor `output` y sus correspondientes índices.
        *   El resto del código formatea e imprime los nombres de las clases correspondientes a estos índices principales.
    *   **Posibles Modificaciones:**
        *   Puedes cambiar el número de predicciones principales cambiando el `3` en `torch.topk(output, 3)`.
        *   La forma de acceder a los nombres de clase (`vgg_classes[str(idx)][1]`) depende de la estructura del archivo JSON.

*   **Celda de Markdown (Probar la Predicción):**
    *   Anima a probar la función con diferentes imágenes.

*   **Celdas de Código (Ejemplos de Predicción):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    readable_prediction("data/doggy_door_images/happy_dog.jpg")
    ```
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    readable_prediction("data/doggy_door_images/brown_bear.jpg")
    ```
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    readable_prediction("data/doggy_door_images/sleepy_cat.jpg")
    ```
    *   **Explicación:** Muestran cómo usar `readable_prediction` con diferentes imágenes de animales.

## 6. Solo Perros (Lógica de la Puerta)

Se implementa la lógica para que la puerta solo se abra para perros.

*   **Celda de Markdown (Reglas de la Puerta):**
    *   Se usan los rangos de categorías: perros (151-268) y gatos (281-285).

*   **Celda de Markdown (Ejercicio `argmax`):**
    *   Pregunta sobre la dimensión a usar con `argmax`. La salida del modelo es `(1, 1000)`, donde la primera dimensión es el lote. Queremos el índice de la clase con la mayor probabilidad *dentro* de ese lote, por lo que se opera sobre la dimensión 1.

*   **Celda de Código (Función `doggy_door` - con FIXME):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    def doggy_door(image_path):
        show_image(image_path)
        image = load_and_process_image(image_path)
        
        model.eval() # Asegurarse de que el modelo está en modo evaluación
        with torch.no_grad(): # No calcular gradientes
            # Obtiene el índice de la clase con la mayor probabilidad/logit
            # La salida de model(image) es (1, 1000). argmax a lo largo de dim=1.
            idx = model(image).argmax(dim=FIXME).item() # FIXME debe ser 1
            
        print("Predicted index:", idx)
        if 151 <= idx <= 268: # Rango para perros
            print("Doggy come on in!")
        elif 281 <= idx <= 285: # Rango para gatos
            print("Kitty stay inside!")
        else:
            print("You're not a dog! Stay outside!")
    ```
    *   **Explicación:**
        *   `model(image).argmax(dim=FIXME).item()`:
            *   `model(image)`: Produce la salida `(1, 1000)`.
            *   `.argmax(dim=1)`: Encuentra el índice del valor máximo a lo largo de la dimensión 1 (la dimensión de las clases). Esto devuelve un tensor con el índice.
            *   `.item()`: Convierte el tensor de un solo elemento (el índice) a un número Python.
        *   El `FIXME` debe ser `1`.
        *   La lógica `if/elif/else` determina la acción basada en el índice predicho.
    *   **Posibles Modificaciones:**
        *   Los rangos de índices `151 <= idx <= 268` y `281 <= idx <= 285` son específicos de ImageNet y VGG16. Si usas otro modelo/dataset, estos cambiarían.

*   **Celda de Markdown (Solución):**
    *   Indica dónde encontrar la solución.

*   **Celda de Código (Solución para `doggy_door`):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    # SOLUTION
    # import numpy as np # No es necesario aquí si se usa torch.argmax().item()

    def doggy_door(image_path):
        show_image(image_path)
        image = load_and_process_image(image_path)
        
        model.eval()
        with torch.no_grad():
            idx = model(image).argmax(dim=1).item() # Solución: dim=1
            
        print("Predicted index:", idx)
        if 151 <= idx <= 268:
            print("Doggy come on in!")
        elif 281 <= idx <= 285:
            print("Kitty stay inside!")
        else:
            print("You're not a dog! Stay outside!")
    ```

*   **Celdas de Código (Pruebas de `doggy_door`):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    doggy_door("data/doggy_door_images/brown_bear.jpg")
    ```
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    doggy_door("data/doggy_door_images/happy_dog.jpg")
    ```
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    doggy_door("data/doggy_door_images/sleepy_cat.jpg")
    ```
    *   **Explicación:** Muestran la función `doggy_door` en acción.

## 7. Resumen y Siguientes Pasos

*   **Celda de Markdown (Resumen):**
    *   Se creó una puerta para perros funcional usando un modelo preentrenado con poco código.
    *   Destaca la ventaja de usar modelos existentes.

*   **Celda de Markdown (Limpiar Memoria):**
    *   Instruye ejecutar una celda para liberar memoria de la GPU.

*   **Celda de Código (Limpiar Memoria):**
    ```python
    # filepath: d:\UNJBG\NOVEMO SEMESTRE\SISTEMAS EXPERTOS\lab-sistemas-expertos\moedlos\05a_doggy_door.ipynb
    import IPython
    app = IPython.Application.instance()
    app.kernel.do_shutdown(True) # Reinicia el kernel de IPython
    ```
    *   **Explicación:** Intenta reiniciar el kernel del notebook para liberar recursos, especialmente la memoria de la GPU.

*   **Celda de Markdown (Siguiente):**
    *   Introduce el siguiente tema: aprendizaje por transferencia (`transfer learning`), que permite adaptar modelos preentrenados a datos específicos.
    *   Enlace al siguiente cuaderno: `05b_presidential_doggy_door.ipynb`.

Este desglose debería ayudarte a entender cada parte del cuaderno y saber dónde y por qué podrías necesitar hacer modificaciones.
